I1106 16:17:09.186961 28978 caffe.cpp:219] Using GPUs 0
I1106 16:17:09.298748 28978 caffe.cpp:224] GPU 0: GeForce GTX 1060 6GB
I1106 16:17:09.601176 28978 solver.cpp:44] Initializing solver from parameters: 
test_iter: 25
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 10000000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "trainedmodels/AlexNet"
solver_mode: GPU
device_id: 0
net: "modeldef/AlexNet/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I1106 16:17:09.629206 28978 solver.cpp:87] Creating training net from net file: modeldef/AlexNet/train_val.prototxt
I1106 16:17:09.661955 28978 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1106 16:17:09.662014 28978 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1106 16:17:09.662475 28978 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "modeldef/mean.binaryproto"
  }
  data_param {
    source: "lmdb/train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_new"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_new"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_new"
  bottom: "label"
  top: "loss"
}
I1106 16:17:09.662765 28978 layer_factory.cpp:63] Creating layer data
I1106 16:17:09.732657 28978 db_lmdb.cpp:40] Opened lmdb lmdb/train_lmdb
I1106 16:17:09.772181 28978 net.cpp:84] Creating Layer data
I1106 16:17:09.772248 28978 net.cpp:380] data -> data
I1106 16:17:09.773633 28978 net.cpp:380] data -> label
I1106 16:17:09.773679 28978 data_transformer.cpp:25] Loading mean file from: modeldef/mean.binaryproto
I1106 16:17:09.873054 28978 data_layer.cpp:45] output data size: 64,3,227,227
I1106 16:17:09.979934 28978 net.cpp:122] Setting up data
I1106 16:17:09.979951 28978 net.cpp:129] Top shape: 64 3 227 227 (9893568)
I1106 16:17:09.979954 28978 net.cpp:129] Top shape: 64 (64)
I1106 16:17:09.979957 28978 net.cpp:137] Memory required for data: 39574528
I1106 16:17:09.979966 28978 layer_factory.cpp:63] Creating layer conv1
I1106 16:17:09.979984 28978 net.cpp:84] Creating Layer conv1
I1106 16:17:09.979988 28978 net.cpp:406] conv1 <- data
I1106 16:17:09.979996 28978 net.cpp:380] conv1 -> conv1
I1106 16:17:14.193675 28978 net.cpp:122] Setting up conv1
I1106 16:17:14.193696 28978 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 16:17:14.193699 28978 net.cpp:137] Memory required for data: 113916928
I1106 16:17:14.193719 28978 layer_factory.cpp:63] Creating layer relu1
I1106 16:17:14.193729 28978 net.cpp:84] Creating Layer relu1
I1106 16:17:14.193732 28978 net.cpp:406] relu1 <- conv1
I1106 16:17:14.193735 28978 net.cpp:367] relu1 -> conv1 (in-place)
I1106 16:17:14.194051 28978 net.cpp:122] Setting up relu1
I1106 16:17:14.194059 28978 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 16:17:14.194061 28978 net.cpp:137] Memory required for data: 188259328
I1106 16:17:14.194063 28978 layer_factory.cpp:63] Creating layer norm1
I1106 16:17:14.194070 28978 net.cpp:84] Creating Layer norm1
I1106 16:17:14.194072 28978 net.cpp:406] norm1 <- conv1
I1106 16:17:14.194077 28978 net.cpp:380] norm1 -> norm1
I1106 16:17:14.194396 28978 net.cpp:122] Setting up norm1
I1106 16:17:14.194403 28978 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 16:17:14.194406 28978 net.cpp:137] Memory required for data: 262601728
I1106 16:17:14.194408 28978 layer_factory.cpp:63] Creating layer pool1
I1106 16:17:14.194412 28978 net.cpp:84] Creating Layer pool1
I1106 16:17:14.194414 28978 net.cpp:406] pool1 <- norm1
I1106 16:17:14.194418 28978 net.cpp:380] pool1 -> pool1
I1106 16:17:14.194458 28978 net.cpp:122] Setting up pool1
I1106 16:17:14.194463 28978 net.cpp:129] Top shape: 64 96 27 27 (4478976)
I1106 16:17:14.194464 28978 net.cpp:137] Memory required for data: 280517632
I1106 16:17:14.194466 28978 layer_factory.cpp:63] Creating layer conv2
I1106 16:17:14.194473 28978 net.cpp:84] Creating Layer conv2
I1106 16:17:14.194475 28978 net.cpp:406] conv2 <- pool1
I1106 16:17:14.194478 28978 net.cpp:380] conv2 -> conv2
I1106 16:17:14.199137 28978 net.cpp:122] Setting up conv2
I1106 16:17:14.199146 28978 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 16:17:14.199148 28978 net.cpp:137] Memory required for data: 328293376
I1106 16:17:14.199154 28978 layer_factory.cpp:63] Creating layer relu2
I1106 16:17:14.199159 28978 net.cpp:84] Creating Layer relu2
I1106 16:17:14.199162 28978 net.cpp:406] relu2 <- conv2
I1106 16:17:14.199164 28978 net.cpp:367] relu2 -> conv2 (in-place)
I1106 16:17:14.199460 28978 net.cpp:122] Setting up relu2
I1106 16:17:14.199466 28978 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 16:17:14.199468 28978 net.cpp:137] Memory required for data: 376069120
I1106 16:17:14.199470 28978 layer_factory.cpp:63] Creating layer norm2
I1106 16:17:14.199476 28978 net.cpp:84] Creating Layer norm2
I1106 16:17:14.199477 28978 net.cpp:406] norm2 <- conv2
I1106 16:17:14.199481 28978 net.cpp:380] norm2 -> norm2
I1106 16:17:14.199690 28978 net.cpp:122] Setting up norm2
I1106 16:17:14.199695 28978 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 16:17:14.199698 28978 net.cpp:137] Memory required for data: 423844864
I1106 16:17:14.199700 28978 layer_factory.cpp:63] Creating layer pool2
I1106 16:17:14.199704 28978 net.cpp:84] Creating Layer pool2
I1106 16:17:14.199707 28978 net.cpp:406] pool2 <- norm2
I1106 16:17:14.199709 28978 net.cpp:380] pool2 -> pool2
I1106 16:17:14.199728 28978 net.cpp:122] Setting up pool2
I1106 16:17:14.199730 28978 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 16:17:14.199733 28978 net.cpp:137] Memory required for data: 434920448
I1106 16:17:14.199735 28978 layer_factory.cpp:63] Creating layer conv3
I1106 16:17:14.199739 28978 net.cpp:84] Creating Layer conv3
I1106 16:17:14.199741 28978 net.cpp:406] conv3 <- pool2
I1106 16:17:14.199744 28978 net.cpp:380] conv3 -> conv3
I1106 16:17:14.208516 28978 net.cpp:122] Setting up conv3
I1106 16:17:14.208534 28978 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 16:17:14.208537 28978 net.cpp:137] Memory required for data: 451533824
I1106 16:17:14.208549 28978 layer_factory.cpp:63] Creating layer relu3
I1106 16:17:14.208555 28978 net.cpp:84] Creating Layer relu3
I1106 16:17:14.208559 28978 net.cpp:406] relu3 <- conv3
I1106 16:17:14.208562 28978 net.cpp:367] relu3 -> conv3 (in-place)
I1106 16:17:14.208773 28978 net.cpp:122] Setting up relu3
I1106 16:17:14.208778 28978 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 16:17:14.208781 28978 net.cpp:137] Memory required for data: 468147200
I1106 16:17:14.208783 28978 layer_factory.cpp:63] Creating layer conv4
I1106 16:17:14.208788 28978 net.cpp:84] Creating Layer conv4
I1106 16:17:14.208791 28978 net.cpp:406] conv4 <- conv3
I1106 16:17:14.208794 28978 net.cpp:380] conv4 -> conv4
I1106 16:17:14.215929 28978 net.cpp:122] Setting up conv4
I1106 16:17:14.215940 28978 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 16:17:14.215943 28978 net.cpp:137] Memory required for data: 484760576
I1106 16:17:14.215948 28978 layer_factory.cpp:63] Creating layer relu4
I1106 16:17:14.215952 28978 net.cpp:84] Creating Layer relu4
I1106 16:17:14.215955 28978 net.cpp:406] relu4 <- conv4
I1106 16:17:14.215960 28978 net.cpp:367] relu4 -> conv4 (in-place)
I1106 16:17:14.216323 28978 net.cpp:122] Setting up relu4
I1106 16:17:14.216329 28978 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 16:17:14.216332 28978 net.cpp:137] Memory required for data: 501373952
I1106 16:17:14.216334 28978 layer_factory.cpp:63] Creating layer conv5
I1106 16:17:14.216341 28978 net.cpp:84] Creating Layer conv5
I1106 16:17:14.216343 28978 net.cpp:406] conv5 <- conv4
I1106 16:17:14.216348 28978 net.cpp:380] conv5 -> conv5
I1106 16:17:14.222509 28978 net.cpp:122] Setting up conv5
I1106 16:17:14.222522 28978 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 16:17:14.222523 28978 net.cpp:137] Memory required for data: 512449536
I1106 16:17:14.222532 28978 layer_factory.cpp:63] Creating layer relu5
I1106 16:17:14.222538 28978 net.cpp:84] Creating Layer relu5
I1106 16:17:14.222541 28978 net.cpp:406] relu5 <- conv5
I1106 16:17:14.222544 28978 net.cpp:367] relu5 -> conv5 (in-place)
I1106 16:17:14.222903 28978 net.cpp:122] Setting up relu5
I1106 16:17:14.222911 28978 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 16:17:14.222913 28978 net.cpp:137] Memory required for data: 523525120
I1106 16:17:14.222915 28978 layer_factory.cpp:63] Creating layer pool5
I1106 16:17:14.222919 28978 net.cpp:84] Creating Layer pool5
I1106 16:17:14.222923 28978 net.cpp:406] pool5 <- conv5
I1106 16:17:14.222926 28978 net.cpp:380] pool5 -> pool5
I1106 16:17:14.222951 28978 net.cpp:122] Setting up pool5
I1106 16:17:14.222955 28978 net.cpp:129] Top shape: 64 256 6 6 (589824)
I1106 16:17:14.222957 28978 net.cpp:137] Memory required for data: 525884416
I1106 16:17:14.222960 28978 layer_factory.cpp:63] Creating layer fc6
I1106 16:17:14.222966 28978 net.cpp:84] Creating Layer fc6
I1106 16:17:14.222968 28978 net.cpp:406] fc6 <- pool5
I1106 16:17:14.222973 28978 net.cpp:380] fc6 -> fc6
I1106 16:17:14.474478 28978 net.cpp:122] Setting up fc6
I1106 16:17:14.474494 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:14.474498 28978 net.cpp:137] Memory required for data: 526932992
I1106 16:17:14.474506 28978 layer_factory.cpp:63] Creating layer relu6
I1106 16:17:14.474514 28978 net.cpp:84] Creating Layer relu6
I1106 16:17:14.474516 28978 net.cpp:406] relu6 <- fc6
I1106 16:17:14.474521 28978 net.cpp:367] relu6 -> fc6 (in-place)
I1106 16:17:14.474836 28978 net.cpp:122] Setting up relu6
I1106 16:17:14.474841 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:14.474843 28978 net.cpp:137] Memory required for data: 527981568
I1106 16:17:14.474845 28978 layer_factory.cpp:63] Creating layer drop6
I1106 16:17:14.474855 28978 net.cpp:84] Creating Layer drop6
I1106 16:17:14.474858 28978 net.cpp:406] drop6 <- fc6
I1106 16:17:14.474860 28978 net.cpp:367] drop6 -> fc6 (in-place)
I1106 16:17:14.474880 28978 net.cpp:122] Setting up drop6
I1106 16:17:14.474884 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:14.474886 28978 net.cpp:137] Memory required for data: 529030144
I1106 16:17:14.474887 28978 layer_factory.cpp:63] Creating layer fc7
I1106 16:17:14.474892 28978 net.cpp:84] Creating Layer fc7
I1106 16:17:14.474895 28978 net.cpp:406] fc7 <- fc6
I1106 16:17:14.474898 28978 net.cpp:380] fc7 -> fc7
I1106 16:17:14.587532 28978 net.cpp:122] Setting up fc7
I1106 16:17:14.587548 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:14.587551 28978 net.cpp:137] Memory required for data: 530078720
I1106 16:17:14.587560 28978 layer_factory.cpp:63] Creating layer relu7
I1106 16:17:14.587568 28978 net.cpp:84] Creating Layer relu7
I1106 16:17:14.587570 28978 net.cpp:406] relu7 <- fc7
I1106 16:17:14.587575 28978 net.cpp:367] relu7 -> fc7 (in-place)
I1106 16:17:14.588027 28978 net.cpp:122] Setting up relu7
I1106 16:17:14.588037 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:14.588038 28978 net.cpp:137] Memory required for data: 531127296
I1106 16:17:14.588042 28978 layer_factory.cpp:63] Creating layer drop7
I1106 16:17:14.588045 28978 net.cpp:84] Creating Layer drop7
I1106 16:17:14.588048 28978 net.cpp:406] drop7 <- fc7
I1106 16:17:14.588050 28978 net.cpp:367] drop7 -> fc7 (in-place)
I1106 16:17:14.588069 28978 net.cpp:122] Setting up drop7
I1106 16:17:14.588073 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:14.588074 28978 net.cpp:137] Memory required for data: 532175872
I1106 16:17:14.588075 28978 layer_factory.cpp:63] Creating layer fc8_new
I1106 16:17:14.588081 28978 net.cpp:84] Creating Layer fc8_new
I1106 16:17:14.588083 28978 net.cpp:406] fc8_new <- fc7
I1106 16:17:14.588086 28978 net.cpp:380] fc8_new -> fc8_new
I1106 16:17:14.588207 28978 net.cpp:122] Setting up fc8_new
I1106 16:17:14.588212 28978 net.cpp:129] Top shape: 64 2 (128)
I1106 16:17:14.588213 28978 net.cpp:137] Memory required for data: 532176384
I1106 16:17:14.588217 28978 layer_factory.cpp:63] Creating layer loss
I1106 16:17:14.588223 28978 net.cpp:84] Creating Layer loss
I1106 16:17:14.588227 28978 net.cpp:406] loss <- fc8_new
I1106 16:17:14.588228 28978 net.cpp:406] loss <- label
I1106 16:17:14.588234 28978 net.cpp:380] loss -> loss
I1106 16:17:14.588243 28978 layer_factory.cpp:63] Creating layer loss
I1106 16:17:14.588660 28978 net.cpp:122] Setting up loss
I1106 16:17:14.588667 28978 net.cpp:129] Top shape: (1)
I1106 16:17:14.588670 28978 net.cpp:132]     with loss weight 1
I1106 16:17:14.588680 28978 net.cpp:137] Memory required for data: 532176388
I1106 16:17:14.588682 28978 net.cpp:198] loss needs backward computation.
I1106 16:17:14.588686 28978 net.cpp:198] fc8_new needs backward computation.
I1106 16:17:14.588688 28978 net.cpp:198] drop7 needs backward computation.
I1106 16:17:14.588690 28978 net.cpp:198] relu7 needs backward computation.
I1106 16:17:14.588692 28978 net.cpp:198] fc7 needs backward computation.
I1106 16:17:14.588694 28978 net.cpp:198] drop6 needs backward computation.
I1106 16:17:14.588696 28978 net.cpp:198] relu6 needs backward computation.
I1106 16:17:14.588698 28978 net.cpp:198] fc6 needs backward computation.
I1106 16:17:14.588701 28978 net.cpp:198] pool5 needs backward computation.
I1106 16:17:14.588702 28978 net.cpp:198] relu5 needs backward computation.
I1106 16:17:14.588704 28978 net.cpp:198] conv5 needs backward computation.
I1106 16:17:14.588706 28978 net.cpp:198] relu4 needs backward computation.
I1106 16:17:14.588708 28978 net.cpp:198] conv4 needs backward computation.
I1106 16:17:14.588711 28978 net.cpp:198] relu3 needs backward computation.
I1106 16:17:14.588712 28978 net.cpp:198] conv3 needs backward computation.
I1106 16:17:14.588714 28978 net.cpp:198] pool2 needs backward computation.
I1106 16:17:14.588717 28978 net.cpp:198] norm2 needs backward computation.
I1106 16:17:14.588719 28978 net.cpp:198] relu2 needs backward computation.
I1106 16:17:14.588721 28978 net.cpp:198] conv2 needs backward computation.
I1106 16:17:14.588723 28978 net.cpp:198] pool1 needs backward computation.
I1106 16:17:14.588726 28978 net.cpp:198] norm1 needs backward computation.
I1106 16:17:14.588727 28978 net.cpp:198] relu1 needs backward computation.
I1106 16:17:14.588729 28978 net.cpp:198] conv1 needs backward computation.
I1106 16:17:14.588732 28978 net.cpp:200] data does not need backward computation.
I1106 16:17:14.588733 28978 net.cpp:242] This network produces output loss
I1106 16:17:14.588743 28978 net.cpp:255] Network initialization done.
I1106 16:17:14.596210 28978 solver.cpp:172] Creating test net (#0) specified by net file: modeldef/AlexNet/train_val.prototxt
I1106 16:17:14.596240 28978 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1106 16:17:14.596340 28978 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "modeldef/mean.binaryproto"
  }
  data_param {
    source: "lmdb/val_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_new"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_new"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_new"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_new"
  bottom: "label"
  top: "loss"
}
I1106 16:17:14.596421 28978 layer_factory.cpp:63] Creating layer data
I1106 16:17:14.630957 28978 db_lmdb.cpp:40] Opened lmdb lmdb/val_lmdb
I1106 16:17:14.650171 28978 net.cpp:84] Creating Layer data
I1106 16:17:14.650187 28978 net.cpp:380] data -> data
I1106 16:17:14.650197 28978 net.cpp:380] data -> label
I1106 16:17:14.650203 28978 data_transformer.cpp:25] Loading mean file from: modeldef/mean.binaryproto
I1106 16:17:14.676098 28978 data_layer.cpp:45] output data size: 64,3,227,227
I1106 16:17:14.720690 28978 net.cpp:122] Setting up data
I1106 16:17:14.720706 28978 net.cpp:129] Top shape: 64 3 227 227 (9893568)
I1106 16:17:14.720710 28978 net.cpp:129] Top shape: 64 (64)
I1106 16:17:14.720711 28978 net.cpp:137] Memory required for data: 39574528
I1106 16:17:14.720716 28978 layer_factory.cpp:63] Creating layer label_data_1_split
I1106 16:17:14.720724 28978 net.cpp:84] Creating Layer label_data_1_split
I1106 16:17:14.720727 28978 net.cpp:406] label_data_1_split <- label
I1106 16:17:14.720733 28978 net.cpp:380] label_data_1_split -> label_data_1_split_0
I1106 16:17:14.720742 28978 net.cpp:380] label_data_1_split -> label_data_1_split_1
I1106 16:17:14.720819 28978 net.cpp:122] Setting up label_data_1_split
I1106 16:17:14.720824 28978 net.cpp:129] Top shape: 64 (64)
I1106 16:17:14.720826 28978 net.cpp:129] Top shape: 64 (64)
I1106 16:17:14.720829 28978 net.cpp:137] Memory required for data: 39575040
I1106 16:17:14.720830 28978 layer_factory.cpp:63] Creating layer conv1
I1106 16:17:14.720839 28978 net.cpp:84] Creating Layer conv1
I1106 16:17:14.720840 28978 net.cpp:406] conv1 <- data
I1106 16:17:14.720844 28978 net.cpp:380] conv1 -> conv1
I1106 16:17:14.725024 28978 net.cpp:122] Setting up conv1
I1106 16:17:14.725031 28978 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 16:17:14.725033 28978 net.cpp:137] Memory required for data: 113917440
I1106 16:17:14.725040 28978 layer_factory.cpp:63] Creating layer relu1
I1106 16:17:14.725044 28978 net.cpp:84] Creating Layer relu1
I1106 16:17:14.725046 28978 net.cpp:406] relu1 <- conv1
I1106 16:17:14.725049 28978 net.cpp:367] relu1 -> conv1 (in-place)
I1106 16:17:14.725359 28978 net.cpp:122] Setting up relu1
I1106 16:17:14.725365 28978 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 16:17:14.725368 28978 net.cpp:137] Memory required for data: 188259840
I1106 16:17:14.725369 28978 layer_factory.cpp:63] Creating layer norm1
I1106 16:17:14.725374 28978 net.cpp:84] Creating Layer norm1
I1106 16:17:14.725376 28978 net.cpp:406] norm1 <- conv1
I1106 16:17:14.725380 28978 net.cpp:380] norm1 -> norm1
I1106 16:17:14.725694 28978 net.cpp:122] Setting up norm1
I1106 16:17:14.725700 28978 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 16:17:14.725703 28978 net.cpp:137] Memory required for data: 262602240
I1106 16:17:14.725704 28978 layer_factory.cpp:63] Creating layer pool1
I1106 16:17:14.725709 28978 net.cpp:84] Creating Layer pool1
I1106 16:17:14.725711 28978 net.cpp:406] pool1 <- norm1
I1106 16:17:14.725714 28978 net.cpp:380] pool1 -> pool1
I1106 16:17:14.725734 28978 net.cpp:122] Setting up pool1
I1106 16:17:14.725737 28978 net.cpp:129] Top shape: 64 96 27 27 (4478976)
I1106 16:17:14.725739 28978 net.cpp:137] Memory required for data: 280518144
I1106 16:17:14.725741 28978 layer_factory.cpp:63] Creating layer conv2
I1106 16:17:14.725746 28978 net.cpp:84] Creating Layer conv2
I1106 16:17:14.725749 28978 net.cpp:406] conv2 <- pool1
I1106 16:17:14.725751 28978 net.cpp:380] conv2 -> conv2
I1106 16:17:14.730904 28978 net.cpp:122] Setting up conv2
I1106 16:17:14.730916 28978 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 16:17:14.730919 28978 net.cpp:137] Memory required for data: 328293888
I1106 16:17:14.730929 28978 layer_factory.cpp:63] Creating layer relu2
I1106 16:17:14.730935 28978 net.cpp:84] Creating Layer relu2
I1106 16:17:14.730939 28978 net.cpp:406] relu2 <- conv2
I1106 16:17:14.730944 28978 net.cpp:367] relu2 -> conv2 (in-place)
I1106 16:17:14.731390 28978 net.cpp:122] Setting up relu2
I1106 16:17:14.731397 28978 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 16:17:14.731400 28978 net.cpp:137] Memory required for data: 376069632
I1106 16:17:14.731415 28978 layer_factory.cpp:63] Creating layer norm2
I1106 16:17:14.731422 28978 net.cpp:84] Creating Layer norm2
I1106 16:17:14.731424 28978 net.cpp:406] norm2 <- conv2
I1106 16:17:14.731428 28978 net.cpp:380] norm2 -> norm2
I1106 16:17:14.731712 28978 net.cpp:122] Setting up norm2
I1106 16:17:14.731719 28978 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 16:17:14.731720 28978 net.cpp:137] Memory required for data: 423845376
I1106 16:17:14.731722 28978 layer_factory.cpp:63] Creating layer pool2
I1106 16:17:14.731729 28978 net.cpp:84] Creating Layer pool2
I1106 16:17:14.731731 28978 net.cpp:406] pool2 <- norm2
I1106 16:17:14.731734 28978 net.cpp:380] pool2 -> pool2
I1106 16:17:14.731756 28978 net.cpp:122] Setting up pool2
I1106 16:17:14.731760 28978 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 16:17:14.731761 28978 net.cpp:137] Memory required for data: 434920960
I1106 16:17:14.731763 28978 layer_factory.cpp:63] Creating layer conv3
I1106 16:17:14.731770 28978 net.cpp:84] Creating Layer conv3
I1106 16:17:14.731771 28978 net.cpp:406] conv3 <- pool2
I1106 16:17:14.731776 28978 net.cpp:380] conv3 -> conv3
I1106 16:17:14.739691 28978 net.cpp:122] Setting up conv3
I1106 16:17:14.739703 28978 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 16:17:14.739706 28978 net.cpp:137] Memory required for data: 451534336
I1106 16:17:14.739714 28978 layer_factory.cpp:63] Creating layer relu3
I1106 16:17:14.739720 28978 net.cpp:84] Creating Layer relu3
I1106 16:17:14.739723 28978 net.cpp:406] relu3 <- conv3
I1106 16:17:14.739727 28978 net.cpp:367] relu3 -> conv3 (in-place)
I1106 16:17:14.740094 28978 net.cpp:122] Setting up relu3
I1106 16:17:14.740101 28978 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 16:17:14.740103 28978 net.cpp:137] Memory required for data: 468147712
I1106 16:17:14.740105 28978 layer_factory.cpp:63] Creating layer conv4
I1106 16:17:14.740113 28978 net.cpp:84] Creating Layer conv4
I1106 16:17:14.740115 28978 net.cpp:406] conv4 <- conv3
I1106 16:17:14.740119 28978 net.cpp:380] conv4 -> conv4
I1106 16:17:14.746798 28978 net.cpp:122] Setting up conv4
I1106 16:17:14.746807 28978 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 16:17:14.746809 28978 net.cpp:137] Memory required for data: 484761088
I1106 16:17:14.746814 28978 layer_factory.cpp:63] Creating layer relu4
I1106 16:17:14.746819 28978 net.cpp:84] Creating Layer relu4
I1106 16:17:14.746821 28978 net.cpp:406] relu4 <- conv4
I1106 16:17:14.746824 28978 net.cpp:367] relu4 -> conv4 (in-place)
I1106 16:17:14.747189 28978 net.cpp:122] Setting up relu4
I1106 16:17:14.747195 28978 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 16:17:14.747197 28978 net.cpp:137] Memory required for data: 501374464
I1106 16:17:14.747200 28978 layer_factory.cpp:63] Creating layer conv5
I1106 16:17:14.747205 28978 net.cpp:84] Creating Layer conv5
I1106 16:17:14.747208 28978 net.cpp:406] conv5 <- conv4
I1106 16:17:14.747213 28978 net.cpp:380] conv5 -> conv5
I1106 16:17:14.753423 28978 net.cpp:122] Setting up conv5
I1106 16:17:14.753437 28978 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 16:17:14.753439 28978 net.cpp:137] Memory required for data: 512450048
I1106 16:17:14.753449 28978 layer_factory.cpp:63] Creating layer relu5
I1106 16:17:14.753458 28978 net.cpp:84] Creating Layer relu5
I1106 16:17:14.753463 28978 net.cpp:406] relu5 <- conv5
I1106 16:17:14.753468 28978 net.cpp:367] relu5 -> conv5 (in-place)
I1106 16:17:14.753839 28978 net.cpp:122] Setting up relu5
I1106 16:17:14.753844 28978 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 16:17:14.753846 28978 net.cpp:137] Memory required for data: 523525632
I1106 16:17:14.753849 28978 layer_factory.cpp:63] Creating layer pool5
I1106 16:17:14.753855 28978 net.cpp:84] Creating Layer pool5
I1106 16:17:14.753859 28978 net.cpp:406] pool5 <- conv5
I1106 16:17:14.753861 28978 net.cpp:380] pool5 -> pool5
I1106 16:17:14.753890 28978 net.cpp:122] Setting up pool5
I1106 16:17:14.753893 28978 net.cpp:129] Top shape: 64 256 6 6 (589824)
I1106 16:17:14.753895 28978 net.cpp:137] Memory required for data: 525884928
I1106 16:17:14.753909 28978 layer_factory.cpp:63] Creating layer fc6
I1106 16:17:14.753914 28978 net.cpp:84] Creating Layer fc6
I1106 16:17:14.753916 28978 net.cpp:406] fc6 <- pool5
I1106 16:17:14.753921 28978 net.cpp:380] fc6 -> fc6
I1106 16:17:15.028178 28978 net.cpp:122] Setting up fc6
I1106 16:17:15.028193 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:15.028196 28978 net.cpp:137] Memory required for data: 526933504
I1106 16:17:15.028203 28978 layer_factory.cpp:63] Creating layer relu6
I1106 16:17:15.028209 28978 net.cpp:84] Creating Layer relu6
I1106 16:17:15.028213 28978 net.cpp:406] relu6 <- fc6
I1106 16:17:15.028220 28978 net.cpp:367] relu6 -> fc6 (in-place)
I1106 16:17:15.028685 28978 net.cpp:122] Setting up relu6
I1106 16:17:15.028692 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:15.028694 28978 net.cpp:137] Memory required for data: 527982080
I1106 16:17:15.028697 28978 layer_factory.cpp:63] Creating layer drop6
I1106 16:17:15.028702 28978 net.cpp:84] Creating Layer drop6
I1106 16:17:15.028703 28978 net.cpp:406] drop6 <- fc6
I1106 16:17:15.028707 28978 net.cpp:367] drop6 -> fc6 (in-place)
I1106 16:17:15.028726 28978 net.cpp:122] Setting up drop6
I1106 16:17:15.028729 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:15.028731 28978 net.cpp:137] Memory required for data: 529030656
I1106 16:17:15.028733 28978 layer_factory.cpp:63] Creating layer fc7
I1106 16:17:15.028738 28978 net.cpp:84] Creating Layer fc7
I1106 16:17:15.028739 28978 net.cpp:406] fc7 <- fc6
I1106 16:17:15.028743 28978 net.cpp:380] fc7 -> fc7
I1106 16:17:15.141000 28978 net.cpp:122] Setting up fc7
I1106 16:17:15.141016 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:15.141018 28978 net.cpp:137] Memory required for data: 530079232
I1106 16:17:15.141024 28978 layer_factory.cpp:63] Creating layer relu7
I1106 16:17:15.141031 28978 net.cpp:84] Creating Layer relu7
I1106 16:17:15.141036 28978 net.cpp:406] relu7 <- fc7
I1106 16:17:15.141043 28978 net.cpp:367] relu7 -> fc7 (in-place)
I1106 16:17:15.141356 28978 net.cpp:122] Setting up relu7
I1106 16:17:15.141362 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:15.141366 28978 net.cpp:137] Memory required for data: 531127808
I1106 16:17:15.141367 28978 layer_factory.cpp:63] Creating layer drop7
I1106 16:17:15.141371 28978 net.cpp:84] Creating Layer drop7
I1106 16:17:15.141373 28978 net.cpp:406] drop7 <- fc7
I1106 16:17:15.141377 28978 net.cpp:367] drop7 -> fc7 (in-place)
I1106 16:17:15.141396 28978 net.cpp:122] Setting up drop7
I1106 16:17:15.141398 28978 net.cpp:129] Top shape: 64 4096 (262144)
I1106 16:17:15.141399 28978 net.cpp:137] Memory required for data: 532176384
I1106 16:17:15.141402 28978 layer_factory.cpp:63] Creating layer fc8_new
I1106 16:17:15.141407 28978 net.cpp:84] Creating Layer fc8_new
I1106 16:17:15.141409 28978 net.cpp:406] fc8_new <- fc7
I1106 16:17:15.141412 28978 net.cpp:380] fc8_new -> fc8_new
I1106 16:17:15.141528 28978 net.cpp:122] Setting up fc8_new
I1106 16:17:15.141532 28978 net.cpp:129] Top shape: 64 2 (128)
I1106 16:17:15.141535 28978 net.cpp:137] Memory required for data: 532176896
I1106 16:17:15.141537 28978 layer_factory.cpp:63] Creating layer fc8_new_fc8_new_0_split
I1106 16:17:15.141541 28978 net.cpp:84] Creating Layer fc8_new_fc8_new_0_split
I1106 16:17:15.141543 28978 net.cpp:406] fc8_new_fc8_new_0_split <- fc8_new
I1106 16:17:15.141546 28978 net.cpp:380] fc8_new_fc8_new_0_split -> fc8_new_fc8_new_0_split_0
I1106 16:17:15.141551 28978 net.cpp:380] fc8_new_fc8_new_0_split -> fc8_new_fc8_new_0_split_1
I1106 16:17:15.141573 28978 net.cpp:122] Setting up fc8_new_fc8_new_0_split
I1106 16:17:15.141577 28978 net.cpp:129] Top shape: 64 2 (128)
I1106 16:17:15.141578 28978 net.cpp:129] Top shape: 64 2 (128)
I1106 16:17:15.141580 28978 net.cpp:137] Memory required for data: 532177920
I1106 16:17:15.141582 28978 layer_factory.cpp:63] Creating layer accuracy
I1106 16:17:15.141587 28978 net.cpp:84] Creating Layer accuracy
I1106 16:17:15.141589 28978 net.cpp:406] accuracy <- fc8_new_fc8_new_0_split_0
I1106 16:17:15.141592 28978 net.cpp:406] accuracy <- label_data_1_split_0
I1106 16:17:15.141605 28978 net.cpp:380] accuracy -> accuracy
I1106 16:17:15.141610 28978 net.cpp:122] Setting up accuracy
I1106 16:17:15.141613 28978 net.cpp:129] Top shape: (1)
I1106 16:17:15.141614 28978 net.cpp:137] Memory required for data: 532177924
I1106 16:17:15.141616 28978 layer_factory.cpp:63] Creating layer loss
I1106 16:17:15.141619 28978 net.cpp:84] Creating Layer loss
I1106 16:17:15.141621 28978 net.cpp:406] loss <- fc8_new_fc8_new_0_split_1
I1106 16:17:15.141624 28978 net.cpp:406] loss <- label_data_1_split_1
I1106 16:17:15.141628 28978 net.cpp:380] loss -> loss
I1106 16:17:15.141631 28978 layer_factory.cpp:63] Creating layer loss
I1106 16:17:15.142102 28978 net.cpp:122] Setting up loss
I1106 16:17:15.142107 28978 net.cpp:129] Top shape: (1)
I1106 16:17:15.142109 28978 net.cpp:132]     with loss weight 1
I1106 16:17:15.142115 28978 net.cpp:137] Memory required for data: 532177928
I1106 16:17:15.142117 28978 net.cpp:198] loss needs backward computation.
I1106 16:17:15.142120 28978 net.cpp:200] accuracy does not need backward computation.
I1106 16:17:15.142123 28978 net.cpp:198] fc8_new_fc8_new_0_split needs backward computation.
I1106 16:17:15.142125 28978 net.cpp:198] fc8_new needs backward computation.
I1106 16:17:15.142127 28978 net.cpp:198] drop7 needs backward computation.
I1106 16:17:15.142129 28978 net.cpp:198] relu7 needs backward computation.
I1106 16:17:15.142132 28978 net.cpp:198] fc7 needs backward computation.
I1106 16:17:15.142133 28978 net.cpp:198] drop6 needs backward computation.
I1106 16:17:15.142134 28978 net.cpp:198] relu6 needs backward computation.
I1106 16:17:15.142138 28978 net.cpp:198] fc6 needs backward computation.
I1106 16:17:15.142139 28978 net.cpp:198] pool5 needs backward computation.
I1106 16:17:15.142141 28978 net.cpp:198] relu5 needs backward computation.
I1106 16:17:15.142143 28978 net.cpp:198] conv5 needs backward computation.
I1106 16:17:15.142145 28978 net.cpp:198] relu4 needs backward computation.
I1106 16:17:15.142148 28978 net.cpp:198] conv4 needs backward computation.
I1106 16:17:15.142149 28978 net.cpp:198] relu3 needs backward computation.
I1106 16:17:15.142154 28978 net.cpp:198] conv3 needs backward computation.
I1106 16:17:15.142158 28978 net.cpp:198] pool2 needs backward computation.
I1106 16:17:15.142159 28978 net.cpp:198] norm2 needs backward computation.
I1106 16:17:15.142161 28978 net.cpp:198] relu2 needs backward computation.
I1106 16:17:15.142163 28978 net.cpp:198] conv2 needs backward computation.
I1106 16:17:15.142165 28978 net.cpp:198] pool1 needs backward computation.
I1106 16:17:15.142168 28978 net.cpp:198] norm1 needs backward computation.
I1106 16:17:15.142170 28978 net.cpp:198] relu1 needs backward computation.
I1106 16:17:15.142171 28978 net.cpp:198] conv1 needs backward computation.
I1106 16:17:15.142175 28978 net.cpp:200] label_data_1_split does not need backward computation.
I1106 16:17:15.142179 28978 net.cpp:200] data does not need backward computation.
I1106 16:17:15.142179 28978 net.cpp:242] This network produces output accuracy
I1106 16:17:15.142182 28978 net.cpp:242] This network produces output loss
I1106 16:17:15.142192 28978 net.cpp:255] Network initialization done.
I1106 16:17:15.142235 28978 solver.cpp:56] Solver scaffolding done.
I1106 16:17:15.142547 28978 caffe.cpp:249] Starting Optimization
I1106 16:17:15.142551 28978 solver.cpp:272] Solving AlexNet
I1106 16:17:15.142552 28978 solver.cpp:273] Learning Rate Policy: poly
I1106 16:17:15.143416 28978 solver.cpp:330] Iteration 0, Testing net (#0)
I1106 16:17:15.226233 28978 blocking_queue.cpp:49] Waiting for data
I1106 16:17:23.725633 28978 solver.cpp:397]     Test net output #0: accuracy = 0.5025
I1106 16:17:23.725674 28978 solver.cpp:397]     Test net output #1: loss = 0.692805 (* 1 = 0.692805 loss)
I1106 16:17:23.855180 28978 solver.cpp:218] Iteration 0 (4.88221e+36 iter/s, 8.71255s/100 iters), loss = 0.696889
I1106 16:17:23.855211 28978 solver.cpp:237]     Train net output #0: loss = 0.696889 (* 1 = 0.696889 loss)
I1106 16:17:23.855245 28978 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I1106 16:17:59.855312 28978 solver.cpp:218] Iteration 100 (2.77779 iter/s, 35.9999s/100 iters), loss = 0.685749
I1106 16:17:59.855371 28978 solver.cpp:237]     Train net output #0: loss = 0.685749 (* 1 = 0.685749 loss)
I1106 16:17:59.855378 28978 sgd_solver.cpp:105] Iteration 100, lr = 0.00099999
