I1106 17:29:30.827625  9276 caffe.cpp:219] Using GPUs 0
I1106 17:29:30.847750  9276 caffe.cpp:224] GPU 0: GeForce GTX 1060 6GB
I1106 17:29:31.006546  9276 solver.cpp:44] Initializing solver from parameters: 
test_iter: 25
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 100000
lr_policy: "poly"
power: 1
momentum: 0.9
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "trainedmodels/AlexNet"
solver_mode: GPU
device_id: 0
net: "modeldef/AlexNet/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I1106 17:29:31.007050  9276 solver.cpp:87] Creating training net from net file: modeldef/AlexNet/train_val.prototxt
I1106 17:29:31.007674  9276 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1106 17:29:31.007774  9276 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "modeldef/mean.binaryproto"
  }
  data_param {
    source: "lmdb/train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_new"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_new"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_new"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_new"
  bottom: "label"
  top: "loss"
}
I1106 17:29:31.007848  9276 layer_factory.cpp:63] Creating layer data
I1106 17:29:31.008829  9276 db_lmdb.cpp:40] Opened lmdb lmdb/train_lmdb
I1106 17:29:31.008852  9276 net.cpp:84] Creating Layer data
I1106 17:29:31.008857  9276 net.cpp:380] data -> data
I1106 17:29:31.008879  9276 net.cpp:380] data -> label
I1106 17:29:31.008889  9276 data_transformer.cpp:25] Loading mean file from: modeldef/mean.binaryproto
I1106 17:29:31.010713  9276 data_layer.cpp:45] output data size: 64,3,227,227
I1106 17:29:31.056041  9276 net.cpp:122] Setting up data
I1106 17:29:31.056059  9276 net.cpp:129] Top shape: 64 3 227 227 (9893568)
I1106 17:29:31.056062  9276 net.cpp:129] Top shape: 64 (64)
I1106 17:29:31.056064  9276 net.cpp:137] Memory required for data: 39574528
I1106 17:29:31.056072  9276 layer_factory.cpp:63] Creating layer label_data_1_split
I1106 17:29:31.056083  9276 net.cpp:84] Creating Layer label_data_1_split
I1106 17:29:31.056087  9276 net.cpp:406] label_data_1_split <- label
I1106 17:29:31.056095  9276 net.cpp:380] label_data_1_split -> label_data_1_split_0
I1106 17:29:31.056102  9276 net.cpp:380] label_data_1_split -> label_data_1_split_1
I1106 17:29:31.056159  9276 net.cpp:122] Setting up label_data_1_split
I1106 17:29:31.056164  9276 net.cpp:129] Top shape: 64 (64)
I1106 17:29:31.056165  9276 net.cpp:129] Top shape: 64 (64)
I1106 17:29:31.056169  9276 net.cpp:137] Memory required for data: 39575040
I1106 17:29:31.056170  9276 layer_factory.cpp:63] Creating layer conv1
I1106 17:29:31.056181  9276 net.cpp:84] Creating Layer conv1
I1106 17:29:31.056185  9276 net.cpp:406] conv1 <- data
I1106 17:29:31.056188  9276 net.cpp:380] conv1 -> conv1
I1106 17:29:31.468801  9276 net.cpp:122] Setting up conv1
I1106 17:29:31.468822  9276 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 17:29:31.468824  9276 net.cpp:137] Memory required for data: 113917440
I1106 17:29:31.468842  9276 layer_factory.cpp:63] Creating layer relu1
I1106 17:29:31.468849  9276 net.cpp:84] Creating Layer relu1
I1106 17:29:31.468852  9276 net.cpp:406] relu1 <- conv1
I1106 17:29:31.468856  9276 net.cpp:367] relu1 -> conv1 (in-place)
I1106 17:29:31.469175  9276 net.cpp:122] Setting up relu1
I1106 17:29:31.469184  9276 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 17:29:31.469187  9276 net.cpp:137] Memory required for data: 188259840
I1106 17:29:31.469189  9276 layer_factory.cpp:63] Creating layer norm1
I1106 17:29:31.469210  9276 net.cpp:84] Creating Layer norm1
I1106 17:29:31.469213  9276 net.cpp:406] norm1 <- conv1
I1106 17:29:31.469218  9276 net.cpp:380] norm1 -> norm1
I1106 17:29:31.469538  9276 net.cpp:122] Setting up norm1
I1106 17:29:31.469547  9276 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 17:29:31.469549  9276 net.cpp:137] Memory required for data: 262602240
I1106 17:29:31.469552  9276 layer_factory.cpp:63] Creating layer pool1
I1106 17:29:31.469555  9276 net.cpp:84] Creating Layer pool1
I1106 17:29:31.469558  9276 net.cpp:406] pool1 <- norm1
I1106 17:29:31.469561  9276 net.cpp:380] pool1 -> pool1
I1106 17:29:31.469586  9276 net.cpp:122] Setting up pool1
I1106 17:29:31.469589  9276 net.cpp:129] Top shape: 64 96 27 27 (4478976)
I1106 17:29:31.469592  9276 net.cpp:137] Memory required for data: 280518144
I1106 17:29:31.469594  9276 layer_factory.cpp:63] Creating layer conv2
I1106 17:29:31.469600  9276 net.cpp:84] Creating Layer conv2
I1106 17:29:31.469602  9276 net.cpp:406] conv2 <- pool1
I1106 17:29:31.469606  9276 net.cpp:380] conv2 -> conv2
I1106 17:29:31.474393  9276 net.cpp:122] Setting up conv2
I1106 17:29:31.474407  9276 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 17:29:31.474411  9276 net.cpp:137] Memory required for data: 328293888
I1106 17:29:31.474418  9276 layer_factory.cpp:63] Creating layer relu2
I1106 17:29:31.474424  9276 net.cpp:84] Creating Layer relu2
I1106 17:29:31.474426  9276 net.cpp:406] relu2 <- conv2
I1106 17:29:31.474431  9276 net.cpp:367] relu2 -> conv2 (in-place)
I1106 17:29:31.474792  9276 net.cpp:122] Setting up relu2
I1106 17:29:31.474800  9276 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 17:29:31.474803  9276 net.cpp:137] Memory required for data: 376069632
I1106 17:29:31.474805  9276 layer_factory.cpp:63] Creating layer norm2
I1106 17:29:31.474812  9276 net.cpp:84] Creating Layer norm2
I1106 17:29:31.474814  9276 net.cpp:406] norm2 <- conv2
I1106 17:29:31.474818  9276 net.cpp:380] norm2 -> norm2
I1106 17:29:31.475093  9276 net.cpp:122] Setting up norm2
I1106 17:29:31.475101  9276 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 17:29:31.475103  9276 net.cpp:137] Memory required for data: 423845376
I1106 17:29:31.475106  9276 layer_factory.cpp:63] Creating layer pool2
I1106 17:29:31.475109  9276 net.cpp:84] Creating Layer pool2
I1106 17:29:31.475111  9276 net.cpp:406] pool2 <- norm2
I1106 17:29:31.475116  9276 net.cpp:380] pool2 -> pool2
I1106 17:29:31.475136  9276 net.cpp:122] Setting up pool2
I1106 17:29:31.475139  9276 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 17:29:31.475142  9276 net.cpp:137] Memory required for data: 434920960
I1106 17:29:31.475143  9276 layer_factory.cpp:63] Creating layer conv3
I1106 17:29:31.475152  9276 net.cpp:84] Creating Layer conv3
I1106 17:29:31.475153  9276 net.cpp:406] conv3 <- pool2
I1106 17:29:31.475157  9276 net.cpp:380] conv3 -> conv3
I1106 17:29:31.482461  9276 net.cpp:122] Setting up conv3
I1106 17:29:31.482477  9276 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 17:29:31.482479  9276 net.cpp:137] Memory required for data: 451534336
I1106 17:29:31.482488  9276 layer_factory.cpp:63] Creating layer relu3
I1106 17:29:31.482496  9276 net.cpp:84] Creating Layer relu3
I1106 17:29:31.482498  9276 net.cpp:406] relu3 <- conv3
I1106 17:29:31.482503  9276 net.cpp:367] relu3 -> conv3 (in-place)
I1106 17:29:31.482779  9276 net.cpp:122] Setting up relu3
I1106 17:29:31.482785  9276 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 17:29:31.482789  9276 net.cpp:137] Memory required for data: 468147712
I1106 17:29:31.482790  9276 layer_factory.cpp:63] Creating layer conv4
I1106 17:29:31.482798  9276 net.cpp:84] Creating Layer conv4
I1106 17:29:31.482801  9276 net.cpp:406] conv4 <- conv3
I1106 17:29:31.482805  9276 net.cpp:380] conv4 -> conv4
I1106 17:29:31.490219  9276 net.cpp:122] Setting up conv4
I1106 17:29:31.490236  9276 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 17:29:31.490237  9276 net.cpp:137] Memory required for data: 484761088
I1106 17:29:31.490244  9276 layer_factory.cpp:63] Creating layer relu4
I1106 17:29:31.490265  9276 net.cpp:84] Creating Layer relu4
I1106 17:29:31.490268  9276 net.cpp:406] relu4 <- conv4
I1106 17:29:31.490274  9276 net.cpp:367] relu4 -> conv4 (in-place)
I1106 17:29:31.490633  9276 net.cpp:122] Setting up relu4
I1106 17:29:31.490640  9276 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 17:29:31.490643  9276 net.cpp:137] Memory required for data: 501374464
I1106 17:29:31.490645  9276 layer_factory.cpp:63] Creating layer conv5
I1106 17:29:31.490653  9276 net.cpp:84] Creating Layer conv5
I1106 17:29:31.490655  9276 net.cpp:406] conv5 <- conv4
I1106 17:29:31.490660  9276 net.cpp:380] conv5 -> conv5
I1106 17:29:31.496758  9276 net.cpp:122] Setting up conv5
I1106 17:29:31.496775  9276 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 17:29:31.496778  9276 net.cpp:137] Memory required for data: 512450048
I1106 17:29:31.496788  9276 layer_factory.cpp:63] Creating layer relu5
I1106 17:29:31.496794  9276 net.cpp:84] Creating Layer relu5
I1106 17:29:31.496798  9276 net.cpp:406] relu5 <- conv5
I1106 17:29:31.496801  9276 net.cpp:367] relu5 -> conv5 (in-place)
I1106 17:29:31.497169  9276 net.cpp:122] Setting up relu5
I1106 17:29:31.497177  9276 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 17:29:31.497180  9276 net.cpp:137] Memory required for data: 523525632
I1106 17:29:31.497182  9276 layer_factory.cpp:63] Creating layer pool5
I1106 17:29:31.497191  9276 net.cpp:84] Creating Layer pool5
I1106 17:29:31.497195  9276 net.cpp:406] pool5 <- conv5
I1106 17:29:31.497198  9276 net.cpp:380] pool5 -> pool5
I1106 17:29:31.497225  9276 net.cpp:122] Setting up pool5
I1106 17:29:31.497229  9276 net.cpp:129] Top shape: 64 256 6 6 (589824)
I1106 17:29:31.497231  9276 net.cpp:137] Memory required for data: 525884928
I1106 17:29:31.497233  9276 layer_factory.cpp:63] Creating layer fc6
I1106 17:29:31.497241  9276 net.cpp:84] Creating Layer fc6
I1106 17:29:31.497244  9276 net.cpp:406] fc6 <- pool5
I1106 17:29:31.497248  9276 net.cpp:380] fc6 -> fc6
I1106 17:29:31.751428  9276 net.cpp:122] Setting up fc6
I1106 17:29:31.751446  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:31.751448  9276 net.cpp:137] Memory required for data: 526933504
I1106 17:29:31.751456  9276 layer_factory.cpp:63] Creating layer relu6
I1106 17:29:31.751464  9276 net.cpp:84] Creating Layer relu6
I1106 17:29:31.751467  9276 net.cpp:406] relu6 <- fc6
I1106 17:29:31.751472  9276 net.cpp:367] relu6 -> fc6 (in-place)
I1106 17:29:31.751791  9276 net.cpp:122] Setting up relu6
I1106 17:29:31.751798  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:31.751801  9276 net.cpp:137] Memory required for data: 527982080
I1106 17:29:31.751806  9276 layer_factory.cpp:63] Creating layer drop6
I1106 17:29:31.751813  9276 net.cpp:84] Creating Layer drop6
I1106 17:29:31.751816  9276 net.cpp:406] drop6 <- fc6
I1106 17:29:31.751818  9276 net.cpp:367] drop6 -> fc6 (in-place)
I1106 17:29:31.751842  9276 net.cpp:122] Setting up drop6
I1106 17:29:31.751845  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:31.751847  9276 net.cpp:137] Memory required for data: 529030656
I1106 17:29:31.751849  9276 layer_factory.cpp:63] Creating layer fc7
I1106 17:29:31.751855  9276 net.cpp:84] Creating Layer fc7
I1106 17:29:31.751857  9276 net.cpp:406] fc7 <- fc6
I1106 17:29:31.751860  9276 net.cpp:380] fc7 -> fc7
I1106 17:29:31.863745  9276 net.cpp:122] Setting up fc7
I1106 17:29:31.863765  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:31.863766  9276 net.cpp:137] Memory required for data: 530079232
I1106 17:29:31.863775  9276 layer_factory.cpp:63] Creating layer relu7
I1106 17:29:31.863785  9276 net.cpp:84] Creating Layer relu7
I1106 17:29:31.863787  9276 net.cpp:406] relu7 <- fc7
I1106 17:29:31.863792  9276 net.cpp:367] relu7 -> fc7 (in-place)
I1106 17:29:31.864266  9276 net.cpp:122] Setting up relu7
I1106 17:29:31.864274  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:31.864277  9276 net.cpp:137] Memory required for data: 531127808
I1106 17:29:31.864281  9276 layer_factory.cpp:63] Creating layer drop7
I1106 17:29:31.864286  9276 net.cpp:84] Creating Layer drop7
I1106 17:29:31.864301  9276 net.cpp:406] drop7 <- fc7
I1106 17:29:31.864306  9276 net.cpp:367] drop7 -> fc7 (in-place)
I1106 17:29:31.864325  9276 net.cpp:122] Setting up drop7
I1106 17:29:31.864328  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:31.864331  9276 net.cpp:137] Memory required for data: 532176384
I1106 17:29:31.864333  9276 layer_factory.cpp:63] Creating layer fc8_new
I1106 17:29:31.864337  9276 net.cpp:84] Creating Layer fc8_new
I1106 17:29:31.864339  9276 net.cpp:406] fc8_new <- fc7
I1106 17:29:31.864344  9276 net.cpp:380] fc8_new -> fc8_new
I1106 17:29:31.864459  9276 net.cpp:122] Setting up fc8_new
I1106 17:29:31.864464  9276 net.cpp:129] Top shape: 64 2 (128)
I1106 17:29:31.864465  9276 net.cpp:137] Memory required for data: 532176896
I1106 17:29:31.864470  9276 layer_factory.cpp:63] Creating layer fc8_new_fc8_new_0_split
I1106 17:29:31.864473  9276 net.cpp:84] Creating Layer fc8_new_fc8_new_0_split
I1106 17:29:31.864475  9276 net.cpp:406] fc8_new_fc8_new_0_split <- fc8_new
I1106 17:29:31.864480  9276 net.cpp:380] fc8_new_fc8_new_0_split -> fc8_new_fc8_new_0_split_0
I1106 17:29:31.864483  9276 net.cpp:380] fc8_new_fc8_new_0_split -> fc8_new_fc8_new_0_split_1
I1106 17:29:31.864503  9276 net.cpp:122] Setting up fc8_new_fc8_new_0_split
I1106 17:29:31.864507  9276 net.cpp:129] Top shape: 64 2 (128)
I1106 17:29:31.864509  9276 net.cpp:129] Top shape: 64 2 (128)
I1106 17:29:31.864511  9276 net.cpp:137] Memory required for data: 532177920
I1106 17:29:31.864513  9276 layer_factory.cpp:63] Creating layer accuracy
I1106 17:29:31.864516  9276 net.cpp:84] Creating Layer accuracy
I1106 17:29:31.864518  9276 net.cpp:406] accuracy <- fc8_new_fc8_new_0_split_0
I1106 17:29:31.864521  9276 net.cpp:406] accuracy <- label_data_1_split_0
I1106 17:29:31.864524  9276 net.cpp:380] accuracy -> accuracy
I1106 17:29:31.864528  9276 net.cpp:122] Setting up accuracy
I1106 17:29:31.864531  9276 net.cpp:129] Top shape: (1)
I1106 17:29:31.864533  9276 net.cpp:137] Memory required for data: 532177924
I1106 17:29:31.864535  9276 layer_factory.cpp:63] Creating layer loss
I1106 17:29:31.864540  9276 net.cpp:84] Creating Layer loss
I1106 17:29:31.864542  9276 net.cpp:406] loss <- fc8_new_fc8_new_0_split_1
I1106 17:29:31.864544  9276 net.cpp:406] loss <- label_data_1_split_1
I1106 17:29:31.864547  9276 net.cpp:380] loss -> loss
I1106 17:29:31.864559  9276 layer_factory.cpp:63] Creating layer loss
I1106 17:29:31.864980  9276 net.cpp:122] Setting up loss
I1106 17:29:31.864989  9276 net.cpp:129] Top shape: (1)
I1106 17:29:31.864993  9276 net.cpp:132]     with loss weight 1
I1106 17:29:31.865005  9276 net.cpp:137] Memory required for data: 532177928
I1106 17:29:31.865008  9276 net.cpp:198] loss needs backward computation.
I1106 17:29:31.865013  9276 net.cpp:200] accuracy does not need backward computation.
I1106 17:29:31.865016  9276 net.cpp:198] fc8_new_fc8_new_0_split needs backward computation.
I1106 17:29:31.865018  9276 net.cpp:198] fc8_new needs backward computation.
I1106 17:29:31.865020  9276 net.cpp:198] drop7 needs backward computation.
I1106 17:29:31.865022  9276 net.cpp:198] relu7 needs backward computation.
I1106 17:29:31.865025  9276 net.cpp:198] fc7 needs backward computation.
I1106 17:29:31.865026  9276 net.cpp:198] drop6 needs backward computation.
I1106 17:29:31.865027  9276 net.cpp:198] relu6 needs backward computation.
I1106 17:29:31.865029  9276 net.cpp:198] fc6 needs backward computation.
I1106 17:29:31.865032  9276 net.cpp:198] pool5 needs backward computation.
I1106 17:29:31.865034  9276 net.cpp:198] relu5 needs backward computation.
I1106 17:29:31.865036  9276 net.cpp:198] conv5 needs backward computation.
I1106 17:29:31.865037  9276 net.cpp:198] relu4 needs backward computation.
I1106 17:29:31.865041  9276 net.cpp:198] conv4 needs backward computation.
I1106 17:29:31.865042  9276 net.cpp:198] relu3 needs backward computation.
I1106 17:29:31.865044  9276 net.cpp:198] conv3 needs backward computation.
I1106 17:29:31.865046  9276 net.cpp:198] pool2 needs backward computation.
I1106 17:29:31.865056  9276 net.cpp:198] norm2 needs backward computation.
I1106 17:29:31.865057  9276 net.cpp:198] relu2 needs backward computation.
I1106 17:29:31.865059  9276 net.cpp:198] conv2 needs backward computation.
I1106 17:29:31.865061  9276 net.cpp:198] pool1 needs backward computation.
I1106 17:29:31.865064  9276 net.cpp:198] norm1 needs backward computation.
I1106 17:29:31.865065  9276 net.cpp:198] relu1 needs backward computation.
I1106 17:29:31.865067  9276 net.cpp:198] conv1 needs backward computation.
I1106 17:29:31.865069  9276 net.cpp:200] label_data_1_split does not need backward computation.
I1106 17:29:31.865072  9276 net.cpp:200] data does not need backward computation.
I1106 17:29:31.865074  9276 net.cpp:242] This network produces output accuracy
I1106 17:29:31.865077  9276 net.cpp:242] This network produces output loss
I1106 17:29:31.865089  9276 net.cpp:255] Network initialization done.
I1106 17:29:31.865911  9276 solver.cpp:172] Creating test net (#0) specified by net file: modeldef/AlexNet/train_val.prototxt
I1106 17:29:31.865936  9276 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1106 17:29:31.866034  9276 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "modeldef/mean.binaryproto"
  }
  data_param {
    source: "lmdb/val_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_new"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_new"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_new"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_new"
  bottom: "label"
  top: "loss"
}
I1106 17:29:31.866091  9276 layer_factory.cpp:63] Creating layer data
I1106 17:29:31.866830  9276 db_lmdb.cpp:40] Opened lmdb lmdb/val_lmdb
I1106 17:29:31.866843  9276 net.cpp:84] Creating Layer data
I1106 17:29:31.866847  9276 net.cpp:380] data -> data
I1106 17:29:31.866852  9276 net.cpp:380] data -> label
I1106 17:29:31.866859  9276 data_transformer.cpp:25] Loading mean file from: modeldef/mean.binaryproto
I1106 17:29:31.868100  9276 data_layer.cpp:45] output data size: 64,3,227,227
I1106 17:29:31.913298  9276 net.cpp:122] Setting up data
I1106 17:29:31.913319  9276 net.cpp:129] Top shape: 64 3 227 227 (9893568)
I1106 17:29:31.913323  9276 net.cpp:129] Top shape: 64 (64)
I1106 17:29:31.913326  9276 net.cpp:137] Memory required for data: 39574528
I1106 17:29:31.913329  9276 layer_factory.cpp:63] Creating layer label_data_1_split
I1106 17:29:31.913337  9276 net.cpp:84] Creating Layer label_data_1_split
I1106 17:29:31.913341  9276 net.cpp:406] label_data_1_split <- label
I1106 17:29:31.913345  9276 net.cpp:380] label_data_1_split -> label_data_1_split_0
I1106 17:29:31.913352  9276 net.cpp:380] label_data_1_split -> label_data_1_split_1
I1106 17:29:31.913424  9276 net.cpp:122] Setting up label_data_1_split
I1106 17:29:31.913429  9276 net.cpp:129] Top shape: 64 (64)
I1106 17:29:31.913432  9276 net.cpp:129] Top shape: 64 (64)
I1106 17:29:31.913434  9276 net.cpp:137] Memory required for data: 39575040
I1106 17:29:31.913436  9276 layer_factory.cpp:63] Creating layer conv1
I1106 17:29:31.913444  9276 net.cpp:84] Creating Layer conv1
I1106 17:29:31.913446  9276 net.cpp:406] conv1 <- data
I1106 17:29:31.913450  9276 net.cpp:380] conv1 -> conv1
I1106 17:29:31.918210  9276 net.cpp:122] Setting up conv1
I1106 17:29:31.918232  9276 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 17:29:31.918236  9276 net.cpp:137] Memory required for data: 113917440
I1106 17:29:31.918249  9276 layer_factory.cpp:63] Creating layer relu1
I1106 17:29:31.918282  9276 net.cpp:84] Creating Layer relu1
I1106 17:29:31.918287  9276 net.cpp:406] relu1 <- conv1
I1106 17:29:31.918292  9276 net.cpp:367] relu1 -> conv1 (in-place)
I1106 17:29:31.918753  9276 net.cpp:122] Setting up relu1
I1106 17:29:31.918766  9276 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 17:29:31.918769  9276 net.cpp:137] Memory required for data: 188259840
I1106 17:29:31.918772  9276 layer_factory.cpp:63] Creating layer norm1
I1106 17:29:31.918781  9276 net.cpp:84] Creating Layer norm1
I1106 17:29:31.918784  9276 net.cpp:406] norm1 <- conv1
I1106 17:29:31.918790  9276 net.cpp:380] norm1 -> norm1
I1106 17:29:31.919255  9276 net.cpp:122] Setting up norm1
I1106 17:29:31.919266  9276 net.cpp:129] Top shape: 64 96 55 55 (18585600)
I1106 17:29:31.919270  9276 net.cpp:137] Memory required for data: 262602240
I1106 17:29:31.919273  9276 layer_factory.cpp:63] Creating layer pool1
I1106 17:29:31.919281  9276 net.cpp:84] Creating Layer pool1
I1106 17:29:31.919283  9276 net.cpp:406] pool1 <- norm1
I1106 17:29:31.919288  9276 net.cpp:380] pool1 -> pool1
I1106 17:29:31.919319  9276 net.cpp:122] Setting up pool1
I1106 17:29:31.919324  9276 net.cpp:129] Top shape: 64 96 27 27 (4478976)
I1106 17:29:31.919327  9276 net.cpp:137] Memory required for data: 280518144
I1106 17:29:31.919329  9276 layer_factory.cpp:63] Creating layer conv2
I1106 17:29:31.919338  9276 net.cpp:84] Creating Layer conv2
I1106 17:29:31.919342  9276 net.cpp:406] conv2 <- pool1
I1106 17:29:31.919348  9276 net.cpp:380] conv2 -> conv2
I1106 17:29:31.932564  9276 net.cpp:122] Setting up conv2
I1106 17:29:31.932588  9276 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 17:29:31.932592  9276 net.cpp:137] Memory required for data: 328293888
I1106 17:29:31.932605  9276 layer_factory.cpp:63] Creating layer relu2
I1106 17:29:31.932615  9276 net.cpp:84] Creating Layer relu2
I1106 17:29:31.932619  9276 net.cpp:406] relu2 <- conv2
I1106 17:29:31.932624  9276 net.cpp:367] relu2 -> conv2 (in-place)
I1106 17:29:31.933133  9276 net.cpp:122] Setting up relu2
I1106 17:29:31.933142  9276 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 17:29:31.933147  9276 net.cpp:137] Memory required for data: 376069632
I1106 17:29:31.933151  9276 layer_factory.cpp:63] Creating layer norm2
I1106 17:29:31.933161  9276 net.cpp:84] Creating Layer norm2
I1106 17:29:31.933164  9276 net.cpp:406] norm2 <- conv2
I1106 17:29:31.933171  9276 net.cpp:380] norm2 -> norm2
I1106 17:29:31.933563  9276 net.cpp:122] Setting up norm2
I1106 17:29:31.933573  9276 net.cpp:129] Top shape: 64 256 27 27 (11943936)
I1106 17:29:31.933578  9276 net.cpp:137] Memory required for data: 423845376
I1106 17:29:31.933580  9276 layer_factory.cpp:63] Creating layer pool2
I1106 17:29:31.933586  9276 net.cpp:84] Creating Layer pool2
I1106 17:29:31.933589  9276 net.cpp:406] pool2 <- norm2
I1106 17:29:31.933594  9276 net.cpp:380] pool2 -> pool2
I1106 17:29:31.933625  9276 net.cpp:122] Setting up pool2
I1106 17:29:31.933630  9276 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 17:29:31.933634  9276 net.cpp:137] Memory required for data: 434920960
I1106 17:29:31.933636  9276 layer_factory.cpp:63] Creating layer conv3
I1106 17:29:31.933647  9276 net.cpp:84] Creating Layer conv3
I1106 17:29:31.933650  9276 net.cpp:406] conv3 <- pool2
I1106 17:29:31.933656  9276 net.cpp:380] conv3 -> conv3
I1106 17:29:31.947546  9276 net.cpp:122] Setting up conv3
I1106 17:29:31.947562  9276 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 17:29:31.947566  9276 net.cpp:137] Memory required for data: 451534336
I1106 17:29:31.947576  9276 layer_factory.cpp:63] Creating layer relu3
I1106 17:29:31.947582  9276 net.cpp:84] Creating Layer relu3
I1106 17:29:31.947587  9276 net.cpp:406] relu3 <- conv3
I1106 17:29:31.947592  9276 net.cpp:367] relu3 -> conv3 (in-place)
I1106 17:29:31.947980  9276 net.cpp:122] Setting up relu3
I1106 17:29:31.947990  9276 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 17:29:31.947993  9276 net.cpp:137] Memory required for data: 468147712
I1106 17:29:31.947995  9276 layer_factory.cpp:63] Creating layer conv4
I1106 17:29:31.948016  9276 net.cpp:84] Creating Layer conv4
I1106 17:29:31.948019  9276 net.cpp:406] conv4 <- conv3
I1106 17:29:31.948024  9276 net.cpp:380] conv4 -> conv4
I1106 17:29:31.960418  9276 net.cpp:122] Setting up conv4
I1106 17:29:31.960434  9276 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 17:29:31.960438  9276 net.cpp:137] Memory required for data: 484761088
I1106 17:29:31.960444  9276 layer_factory.cpp:63] Creating layer relu4
I1106 17:29:31.960453  9276 net.cpp:84] Creating Layer relu4
I1106 17:29:31.960456  9276 net.cpp:406] relu4 <- conv4
I1106 17:29:31.960461  9276 net.cpp:367] relu4 -> conv4 (in-place)
I1106 17:29:31.960836  9276 net.cpp:122] Setting up relu4
I1106 17:29:31.960844  9276 net.cpp:129] Top shape: 64 384 13 13 (4153344)
I1106 17:29:31.960847  9276 net.cpp:137] Memory required for data: 501374464
I1106 17:29:31.960850  9276 layer_factory.cpp:63] Creating layer conv5
I1106 17:29:31.960857  9276 net.cpp:84] Creating Layer conv5
I1106 17:29:31.960860  9276 net.cpp:406] conv5 <- conv4
I1106 17:29:31.960865  9276 net.cpp:380] conv5 -> conv5
I1106 17:29:31.967110  9276 net.cpp:122] Setting up conv5
I1106 17:29:31.967126  9276 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 17:29:31.967128  9276 net.cpp:137] Memory required for data: 512450048
I1106 17:29:31.967139  9276 layer_factory.cpp:63] Creating layer relu5
I1106 17:29:31.967146  9276 net.cpp:84] Creating Layer relu5
I1106 17:29:31.967149  9276 net.cpp:406] relu5 <- conv5
I1106 17:29:31.967154  9276 net.cpp:367] relu5 -> conv5 (in-place)
I1106 17:29:31.969506  9276 net.cpp:122] Setting up relu5
I1106 17:29:31.969516  9276 net.cpp:129] Top shape: 64 256 13 13 (2768896)
I1106 17:29:31.969518  9276 net.cpp:137] Memory required for data: 523525632
I1106 17:29:31.969521  9276 layer_factory.cpp:63] Creating layer pool5
I1106 17:29:31.969528  9276 net.cpp:84] Creating Layer pool5
I1106 17:29:31.969532  9276 net.cpp:406] pool5 <- conv5
I1106 17:29:31.969535  9276 net.cpp:380] pool5 -> pool5
I1106 17:29:31.969566  9276 net.cpp:122] Setting up pool5
I1106 17:29:31.969569  9276 net.cpp:129] Top shape: 64 256 6 6 (589824)
I1106 17:29:31.969571  9276 net.cpp:137] Memory required for data: 525884928
I1106 17:29:31.969573  9276 layer_factory.cpp:63] Creating layer fc6
I1106 17:29:31.969580  9276 net.cpp:84] Creating Layer fc6
I1106 17:29:31.969583  9276 net.cpp:406] fc6 <- pool5
I1106 17:29:31.969585  9276 net.cpp:380] fc6 -> fc6
I1106 17:29:32.232163  9276 net.cpp:122] Setting up fc6
I1106 17:29:32.232182  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:32.232184  9276 net.cpp:137] Memory required for data: 526933504
I1106 17:29:32.232192  9276 layer_factory.cpp:63] Creating layer relu6
I1106 17:29:32.232198  9276 net.cpp:84] Creating Layer relu6
I1106 17:29:32.232201  9276 net.cpp:406] relu6 <- fc6
I1106 17:29:32.232208  9276 net.cpp:367] relu6 -> fc6 (in-place)
I1106 17:29:32.232761  9276 net.cpp:122] Setting up relu6
I1106 17:29:32.232776  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:32.232779  9276 net.cpp:137] Memory required for data: 527982080
I1106 17:29:32.232784  9276 layer_factory.cpp:63] Creating layer drop6
I1106 17:29:32.232789  9276 net.cpp:84] Creating Layer drop6
I1106 17:29:32.232794  9276 net.cpp:406] drop6 <- fc6
I1106 17:29:32.232800  9276 net.cpp:367] drop6 -> fc6 (in-place)
I1106 17:29:32.232826  9276 net.cpp:122] Setting up drop6
I1106 17:29:32.232831  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:32.232832  9276 net.cpp:137] Memory required for data: 529030656
I1106 17:29:32.232834  9276 layer_factory.cpp:63] Creating layer fc7
I1106 17:29:32.232841  9276 net.cpp:84] Creating Layer fc7
I1106 17:29:32.232843  9276 net.cpp:406] fc7 <- fc6
I1106 17:29:32.232848  9276 net.cpp:380] fc7 -> fc7
I1106 17:29:32.366123  9276 net.cpp:122] Setting up fc7
I1106 17:29:32.366142  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:32.366145  9276 net.cpp:137] Memory required for data: 530079232
I1106 17:29:32.366155  9276 layer_factory.cpp:63] Creating layer relu7
I1106 17:29:32.366179  9276 net.cpp:84] Creating Layer relu7
I1106 17:29:32.366183  9276 net.cpp:406] relu7 <- fc7
I1106 17:29:32.366187  9276 net.cpp:367] relu7 -> fc7 (in-place)
I1106 17:29:32.366540  9276 net.cpp:122] Setting up relu7
I1106 17:29:32.366549  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:32.366551  9276 net.cpp:137] Memory required for data: 531127808
I1106 17:29:32.366554  9276 layer_factory.cpp:63] Creating layer drop7
I1106 17:29:32.366559  9276 net.cpp:84] Creating Layer drop7
I1106 17:29:32.366561  9276 net.cpp:406] drop7 <- fc7
I1106 17:29:32.366566  9276 net.cpp:367] drop7 -> fc7 (in-place)
I1106 17:29:32.366585  9276 net.cpp:122] Setting up drop7
I1106 17:29:32.366590  9276 net.cpp:129] Top shape: 64 4096 (262144)
I1106 17:29:32.366591  9276 net.cpp:137] Memory required for data: 532176384
I1106 17:29:32.366593  9276 layer_factory.cpp:63] Creating layer fc8_new
I1106 17:29:32.366600  9276 net.cpp:84] Creating Layer fc8_new
I1106 17:29:32.366601  9276 net.cpp:406] fc8_new <- fc7
I1106 17:29:32.366606  9276 net.cpp:380] fc8_new -> fc8_new
I1106 17:29:32.366739  9276 net.cpp:122] Setting up fc8_new
I1106 17:29:32.366744  9276 net.cpp:129] Top shape: 64 2 (128)
I1106 17:29:32.366746  9276 net.cpp:137] Memory required for data: 532176896
I1106 17:29:32.366751  9276 layer_factory.cpp:63] Creating layer fc8_new_fc8_new_0_split
I1106 17:29:32.366755  9276 net.cpp:84] Creating Layer fc8_new_fc8_new_0_split
I1106 17:29:32.366757  9276 net.cpp:406] fc8_new_fc8_new_0_split <- fc8_new
I1106 17:29:32.366761  9276 net.cpp:380] fc8_new_fc8_new_0_split -> fc8_new_fc8_new_0_split_0
I1106 17:29:32.366766  9276 net.cpp:380] fc8_new_fc8_new_0_split -> fc8_new_fc8_new_0_split_1
I1106 17:29:32.366788  9276 net.cpp:122] Setting up fc8_new_fc8_new_0_split
I1106 17:29:32.366792  9276 net.cpp:129] Top shape: 64 2 (128)
I1106 17:29:32.366794  9276 net.cpp:129] Top shape: 64 2 (128)
I1106 17:29:32.366796  9276 net.cpp:137] Memory required for data: 532177920
I1106 17:29:32.366798  9276 layer_factory.cpp:63] Creating layer accuracy
I1106 17:29:32.366802  9276 net.cpp:84] Creating Layer accuracy
I1106 17:29:32.366804  9276 net.cpp:406] accuracy <- fc8_new_fc8_new_0_split_0
I1106 17:29:32.366807  9276 net.cpp:406] accuracy <- label_data_1_split_0
I1106 17:29:32.366811  9276 net.cpp:380] accuracy -> accuracy
I1106 17:29:32.366816  9276 net.cpp:122] Setting up accuracy
I1106 17:29:32.366817  9276 net.cpp:129] Top shape: (1)
I1106 17:29:32.366820  9276 net.cpp:137] Memory required for data: 532177924
I1106 17:29:32.366822  9276 layer_factory.cpp:63] Creating layer loss
I1106 17:29:32.366825  9276 net.cpp:84] Creating Layer loss
I1106 17:29:32.366828  9276 net.cpp:406] loss <- fc8_new_fc8_new_0_split_1
I1106 17:29:32.366832  9276 net.cpp:406] loss <- label_data_1_split_1
I1106 17:29:32.366835  9276 net.cpp:380] loss -> loss
I1106 17:29:32.366839  9276 layer_factory.cpp:63] Creating layer loss
I1106 17:29:32.367349  9276 net.cpp:122] Setting up loss
I1106 17:29:32.367359  9276 net.cpp:129] Top shape: (1)
I1106 17:29:32.367363  9276 net.cpp:132]     with loss weight 1
I1106 17:29:32.367370  9276 net.cpp:137] Memory required for data: 532177928
I1106 17:29:32.367373  9276 net.cpp:198] loss needs backward computation.
I1106 17:29:32.367377  9276 net.cpp:200] accuracy does not need backward computation.
I1106 17:29:32.367379  9276 net.cpp:198] fc8_new_fc8_new_0_split needs backward computation.
I1106 17:29:32.367383  9276 net.cpp:198] fc8_new needs backward computation.
I1106 17:29:32.367385  9276 net.cpp:198] drop7 needs backward computation.
I1106 17:29:32.367388  9276 net.cpp:198] relu7 needs backward computation.
I1106 17:29:32.367389  9276 net.cpp:198] fc7 needs backward computation.
I1106 17:29:32.367391  9276 net.cpp:198] drop6 needs backward computation.
I1106 17:29:32.367393  9276 net.cpp:198] relu6 needs backward computation.
I1106 17:29:32.367395  9276 net.cpp:198] fc6 needs backward computation.
I1106 17:29:32.367398  9276 net.cpp:198] pool5 needs backward computation.
I1106 17:29:32.367399  9276 net.cpp:198] relu5 needs backward computation.
I1106 17:29:32.367408  9276 net.cpp:198] conv5 needs backward computation.
I1106 17:29:32.367411  9276 net.cpp:198] relu4 needs backward computation.
I1106 17:29:32.367413  9276 net.cpp:198] conv4 needs backward computation.
I1106 17:29:32.367415  9276 net.cpp:198] relu3 needs backward computation.
I1106 17:29:32.367417  9276 net.cpp:198] conv3 needs backward computation.
I1106 17:29:32.367420  9276 net.cpp:198] pool2 needs backward computation.
I1106 17:29:32.367422  9276 net.cpp:198] norm2 needs backward computation.
I1106 17:29:32.367425  9276 net.cpp:198] relu2 needs backward computation.
I1106 17:29:32.367429  9276 net.cpp:198] conv2 needs backward computation.
I1106 17:29:32.367431  9276 net.cpp:198] pool1 needs backward computation.
I1106 17:29:32.367434  9276 net.cpp:198] norm1 needs backward computation.
I1106 17:29:32.367435  9276 net.cpp:198] relu1 needs backward computation.
I1106 17:29:32.367437  9276 net.cpp:198] conv1 needs backward computation.
I1106 17:29:32.367440  9276 net.cpp:200] label_data_1_split does not need backward computation.
I1106 17:29:32.367444  9276 net.cpp:200] data does not need backward computation.
I1106 17:29:32.367444  9276 net.cpp:242] This network produces output accuracy
I1106 17:29:32.367447  9276 net.cpp:242] This network produces output loss
I1106 17:29:32.367460  9276 net.cpp:255] Network initialization done.
I1106 17:29:32.367509  9276 solver.cpp:56] Solver scaffolding done.
I1106 17:29:32.367843  9276 caffe.cpp:249] Starting Optimization
I1106 17:29:32.367849  9276 solver.cpp:272] Solving AlexNet
I1106 17:29:32.367852  9276 solver.cpp:273] Learning Rate Policy: poly
I1106 17:29:32.368786  9276 solver.cpp:330] Iteration 0, Testing net (#0)
I1106 17:29:33.379566  9276 solver.cpp:397]     Test net output #0: accuracy = 0.4975
I1106 17:29:33.379590  9276 solver.cpp:397]     Test net output #1: loss = 0.693549 (* 1 = 0.693549 loss)
I1106 17:29:33.492031  9276 solver.cpp:218] Iteration 0 (3.78471e+37 iter/s, 1.12414s/100 iters), loss = 0.693635
I1106 17:29:33.492063  9276 solver.cpp:237]     Train net output #0: accuracy = 0.546875
I1106 17:29:33.492070  9276 solver.cpp:237]     Train net output #1: loss = 0.693635 (* 1 = 0.693635 loss)
I1106 17:29:33.492079  9276 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1106 17:29:45.794601  9276 solver.cpp:218] Iteration 100 (8.12844 iter/s, 12.3025s/100 iters), loss = 0.684688
I1106 17:29:45.794631  9276 solver.cpp:237]     Train net output #0: accuracy = 0.625
I1106 17:29:45.794638  9276 solver.cpp:237]     Train net output #1: loss = 0.684688 (* 1 = 0.684688 loss)
I1106 17:29:45.794646  9276 sgd_solver.cpp:105] Iteration 100, lr = 0.0999
I1106 17:29:57.890645  9276 solver.cpp:218] Iteration 200 (8.26722 iter/s, 12.096s/100 iters), loss = 0.657403
I1106 17:29:57.890674  9276 solver.cpp:237]     Train net output #0: accuracy = 0.6875
I1106 17:29:57.890682  9276 solver.cpp:237]     Train net output #1: loss = 0.657403 (* 1 = 0.657403 loss)
I1106 17:29:57.890686  9276 sgd_solver.cpp:105] Iteration 200, lr = 0.0998
I1106 17:30:10.242338  9276 solver.cpp:218] Iteration 300 (8.09611 iter/s, 12.3516s/100 iters), loss = 0.693652
I1106 17:30:10.242395  9276 solver.cpp:237]     Train net output #0: accuracy = 0.453125
I1106 17:30:10.242405  9276 solver.cpp:237]     Train net output #1: loss = 0.693652 (* 1 = 0.693652 loss)
I1106 17:30:10.242410  9276 sgd_solver.cpp:105] Iteration 300, lr = 0.0997
I1106 17:30:11.284173  9286 data_layer.cpp:73] Restarting data prefetching from start.
I1106 17:30:22.648399  9276 solver.cpp:218] Iteration 400 (8.06064 iter/s, 12.406s/100 iters), loss = 0.690814
I1106 17:30:22.648430  9276 solver.cpp:237]     Train net output #0: accuracy = 0.5625
I1106 17:30:22.648439  9276 solver.cpp:237]     Train net output #1: loss = 0.690814 (* 1 = 0.690814 loss)
I1106 17:30:22.648444  9276 sgd_solver.cpp:105] Iteration 400, lr = 0.0996
I1106 17:30:34.769305  9276 solver.cpp:330] Iteration 500, Testing net (#0)
I1106 17:30:35.806522  9276 solver.cpp:397]     Test net output #0: accuracy = 0.505
I1106 17:30:35.806543  9276 solver.cpp:397]     Test net output #1: loss = 0.693878 (* 1 = 0.693878 loss)
I1106 17:30:35.915377  9276 solver.cpp:218] Iteration 500 (7.53756 iter/s, 13.2669s/100 iters), loss = 0.691303
I1106 17:30:35.915410  9276 solver.cpp:237]     Train net output #0: accuracy = 0.53125
I1106 17:30:35.915421  9276 solver.cpp:237]     Train net output #1: loss = 0.691303 (* 1 = 0.691303 loss)
I1106 17:30:35.915427  9276 sgd_solver.cpp:105] Iteration 500, lr = 0.0995
I1106 17:30:48.222059  9276 solver.cpp:218] Iteration 600 (8.12572 iter/s, 12.3066s/100 iters), loss = 0.693792
I1106 17:30:48.222131  9276 solver.cpp:237]     Train net output #0: accuracy = 0.5
I1106 17:30:48.222139  9276 solver.cpp:237]     Train net output #1: loss = 0.693792 (* 1 = 0.693792 loss)
I1106 17:30:48.222144  9276 sgd_solver.cpp:105] Iteration 600, lr = 0.0994
I1106 17:30:50.741441  9286 data_layer.cpp:73] Restarting data prefetching from start.
I1106 17:31:00.607389  9276 solver.cpp:218] Iteration 700 (8.07414 iter/s, 12.3852s/100 iters), loss = 0.701571
I1106 17:31:00.607420  9276 solver.cpp:237]     Train net output #0: accuracy = 0.4375
I1106 17:31:00.607427  9276 solver.cpp:237]     Train net output #1: loss = 0.701571 (* 1 = 0.701571 loss)
I1106 17:31:00.607431  9276 sgd_solver.cpp:105] Iteration 700, lr = 0.0993
I1106 17:31:13.454613  9276 solver.cpp:218] Iteration 800 (7.78383 iter/s, 12.8471s/100 iters), loss = 0.711831
I1106 17:31:13.454644  9276 solver.cpp:237]     Train net output #0: accuracy = 0.40625
I1106 17:31:13.454651  9276 solver.cpp:237]     Train net output #1: loss = 0.711831 (* 1 = 0.711831 loss)
I1106 17:31:13.454655  9276 sgd_solver.cpp:105] Iteration 800, lr = 0.0992
I1106 17:31:25.959704  9276 solver.cpp:218] Iteration 900 (7.99679 iter/s, 12.505s/100 iters), loss = 0.69059
I1106 17:31:25.959753  9276 solver.cpp:237]     Train net output #0: accuracy = 0.578125
I1106 17:31:25.959759  9276 solver.cpp:237]     Train net output #1: loss = 0.69059 (* 1 = 0.69059 loss)
I1106 17:31:25.959764  9276 sgd_solver.cpp:105] Iteration 900, lr = 0.0991
I1106 17:31:30.100504  9286 data_layer.cpp:73] Restarting data prefetching from start.
I1106 17:31:38.167971  9276 solver.cpp:447] Snapshotting to binary proto file trainedmodels/AlexNet_iter_1000.caffemodel
I1106 17:33:19.871811  9276 sgd_solver.cpp:273] Snapshotting solver state to binary proto file trainedmodels/AlexNet_iter_1000.solverstate
I1106 17:34:39.744493  9276 solver.cpp:330] Iteration 1000, Testing net (#0)
I1106 17:34:39.864642  9276 blocking_queue.cpp:49] Waiting for data
I1106 17:34:45.581518  9276 solver.cpp:397]     Test net output #0: accuracy = 0.493125
I1106 17:34:45.581542  9276 solver.cpp:397]     Test net output #1: loss = 0.696947 (* 1 = 0.696947 loss)
I1106 17:34:45.703825  9276 solver.cpp:218] Iteration 1000 (0.500642 iter/s, 199.743s/100 iters), loss = 0.702851
I1106 17:34:45.703856  9276 solver.cpp:237]     Train net output #0: accuracy = 0.453125
I1106 17:34:45.703864  9276 solver.cpp:237]     Train net output #1: loss = 0.702851 (* 1 = 0.702851 loss)
I1106 17:34:45.703871  9276 sgd_solver.cpp:105] Iteration 1000, lr = 0.099
I1106 17:34:57.676169  9276 solver.cpp:218] Iteration 1100 (8.35264 iter/s, 11.9723s/100 iters), loss = 0.696649
I1106 17:34:57.676195  9276 solver.cpp:237]     Train net output #0: accuracy = 0.53125
I1106 17:34:57.676203  9276 solver.cpp:237]     Train net output #1: loss = 0.696649 (* 1 = 0.696649 loss)
I1106 17:34:57.676208  9276 sgd_solver.cpp:105] Iteration 1100, lr = 0.0989
I1106 17:35:09.664629  9276 solver.cpp:218] Iteration 1200 (8.34142 iter/s, 11.9884s/100 iters), loss = 0.694273
I1106 17:35:09.664660  9276 solver.cpp:237]     Train net output #0: accuracy = 0.46875
I1106 17:35:09.664669  9276 solver.cpp:237]     Train net output #1: loss = 0.694273 (* 1 = 0.694273 loss)
I1106 17:35:09.664674  9276 sgd_solver.cpp:105] Iteration 1200, lr = 0.0988
I1106 17:35:19.475255  9286 data_layer.cpp:73] Restarting data prefetching from start.
I1106 17:35:25.948113  9276 solver.cpp:218] Iteration 1300 (6.14123 iter/s, 16.2834s/100 iters), loss = 0.680687
I1106 17:35:25.951125  9276 solver.cpp:237]     Train net output #0: accuracy = 0.578125
I1106 17:35:25.951138  9276 solver.cpp:237]     Train net output #1: loss = 0.680687 (* 1 = 0.680687 loss)
I1106 17:35:25.951143  9276 sgd_solver.cpp:105] Iteration 1300, lr = 0.0987
I1106 17:35:38.067590  9276 solver.cpp:218] Iteration 1400 (8.25327 iter/s, 12.1164s/100 iters), loss = 0.706945
I1106 17:35:38.067620  9276 solver.cpp:237]     Train net output #0: accuracy = 0.46875
I1106 17:35:38.067627  9276 solver.cpp:237]     Train net output #1: loss = 0.706945 (* 1 = 0.706945 loss)
I1106 17:35:38.067632  9276 sgd_solver.cpp:105] Iteration 1400, lr = 0.0986
